import requests
from bs4 import BeautifulSoup
import re

def crawl_google(query):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        # ğŸ‘‰ ë‰´ìŠ¤ íƒ­ì—ì„œ ê²€ìƒ‰
        url = f"https://www.google.com/search?q={query.replace(' ', '+')}&tbm=nws"
        response = requests.get(url, headers=headers)

        print("âœ… ìƒíƒœì½”ë“œ:", response.status_code)
        if response.status_code != 200:
            return "ì›¹ ê²€ìƒ‰ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        soup = BeautifulSoup(response.text, 'html.parser')

        # ğŸ‘‰ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
        snippets = soup.select('div.BNeawe.vvjwJb.AP7Wnd')

        print("âœ… ë‰´ìŠ¤ ê²°ê³¼ ê°œìˆ˜:", len(snippets))
        for s in snippets[:3]:
            print("ğŸ“°", s.get_text())

        if not snippets:
            return f"'{query}'ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”."

        titles = [s.get_text() for s in snippets]
        return ' / '.join(titles[:3])
    except Exception as e:
        print("âŒ ì˜ˆì™¸ ë°œìƒ:", e)
        return f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}"


def clean_texts(text_list):
    seen = set()
    cleaned = []

    for text in text_list:
        text = text.strip()
        text = re.sub(r'\s+', ' ', text)

        if len(text) < 30 or text in seen:
            continue

        if 'ë”ë³´ê¸°' in text or 'Google' in text:
            continue

        seen.add(text)
        cleaned.append(text)

    return cleaned

def summarize(texts, max_sentences=3):
    sentences = []

    for t in texts:
        parts = re.split(r'[.!?]', t)
        parts = [p.strip() for p in parts if len(p.strip()) > 20]
        sentences.extend(parts)

    if not sentences:
        return "ìš”ì•½í•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."

    return ' '.join(sentences[:max_sentences]) + '.'

def get_response(user_message, conversation_history=[]):
    return crawl_google(user_message)